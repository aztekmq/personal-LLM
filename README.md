# Personal-LLM

*A minimal GPT-style language model built from scratch in Python + PyTorch.*

This repository contains a fully working, self-contained GPT-style language model implemented in just a few hundred lines of clean Python code. It is intentionally simple, fully transparent, and designed to teach the internal mechanics of modern LLMs while giving you a strong foundation to build uponâ€”whether your end goal is experimentation, domain-specific training, integration with IBM MQ pipelines, or expanding into serious research-grade architectures.

---

# ğŸŒŸ Highlights

This project includes:

* A simple **character-level tokenizer** (no external tokenizer dependencies)
* **Transformer blocks** with:

  * Multi-head self-attention
  * Feed-forward layers
  * LayerNorm
  * GELU activations
* A complete **autoregressive training loop**
* **Text generation** with sampling + temperature control
* GPU acceleration support (if CUDA is available)
* Clear structure and documentation for easy modification
* Immediate expandability into:

  * Token-level LLMs
  * RAG systems
  * IBM MQâ€“integrated AI agents
  * Fine-tuning pipelines
  * CI/CD model deployment workflows

This is an ideal starting point for:

* Learning how LLMs work internally
* Creating a **personal custom LLM** with your own training corpus
* Building domain-specific models (e.g., IBM MQ, z/OS, DevOps, finance, logs, etc.)
* Experimenting with scaling laws, attention modules, and model training techniques
* Building practical AI tools backed by your own model

---

# ğŸ“‚ Project Structure

```
personal-LLM/
â”‚
â”œâ”€â”€ tiny_llm.py              # Main LLM implementation (model, training, generation)
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ .gitignore               # Ignore venvs, cache, PyTorch artifacts
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_corpus.txt    # Example training text corpus
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ train.sh             # Convenience script to train the model
    â””â”€â”€ generate.sh          # Convenience script to generate text
```

You can replace `data/sample_corpus.txt` with whatever text you want the LLM to learn from (logs, notes, documentation, books, MQ configs, etc.).

---

# ğŸ“˜ What Is a Training Corpus?

The â€œtraining corpusâ€ is simply the text your LLM learns from.

Examples include:

### Simple Example Corpus

```
This is a small demonstration corpus for training a tiny GPT-style model.
```

### IBM MQ Expert Corpus (example)

```
IBM MQ queue managers handle message persistence, channels, logs, and recovery.
AMQ9510: Messages cannot be delivered because the channel is inactive.
Cluster workload balancing distributes traffic across queue managers.
```

### Conversational Corpus

```
User: What is a queue manager?
Assistant: A queue manager is the core IBM MQ component that...
```

The more domain-specific and extensive your corpus, the more knowledgeable your LLM becomes in that area.

---

# ğŸ§  How the Model Works

Under the hood, this project implements a simplified version of the GPT architecture:

* **Token Embeddings** â€“ turn characters into dense vectors
* **Positional Embeddings** â€“ encode the order of characters
* **Transformer Blocks** â€“ repeated layers containing:

  * Multi-head self-attention
  * Feed-forward layers
* **Final Projection Layer** â€“ predicts next-token logits

Even though this is a â€œtinyâ€ model, it reflects the *same architectural blueprint* used by GPT-2, GPT-3, LLaMA, Mistral, and other production LLMsâ€”just scaled down for educational and experimental use.

---

# ğŸ‹ï¸â€â™‚ï¸ Getting Started / Training the Model

## 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

(Optional: create a virtual environment first.)

## 2ï¸âƒ£ Train the model on your corpus

```bash
python tiny_llm.py --text_file data/sample_corpus.txt
```

If you donâ€™t provide a corpus file, the script will use a small built-in one.

## 3ï¸âƒ£ Train *and* generate new text

```bash
python tiny_llm.py --text_file data/sample_corpus.txt --generate
```

After training, the model will produce a text continuation sample such as:

```
=== SAMPLE GENERATION ===
IBM MQ is a robust nterprse messaging midalere that...
```

(The quality increases as your corpus size and model size increase.)

---

# ğŸ› Configuration

Model hyperparameters are defined cleanly in a `Config` dataclass:

```python
Config(
    block_size=128,
    batch_size=64,
    n_embd=256,
    n_head=4,
    n_layer=4,
    dropout=0.1,
    max_iters=2000,
    learning_rate=3e-4
)
```

You can scale these up or down depending on hardware and corpus size.

---

# ğŸ§ª Using Helper Scripts (Optional but Convenient)

### Train:

```bash
./scripts/train.sh
```

### Generate text:

```bash
./scripts/generate.sh 300   # generate 300 tokens
```

---

# ğŸ³ Optional: Docker

Add a `Dockerfile` (example provided in earlier responses), then:

```bash
docker build -t personal-llm .
docker run --rm personal-llm
```

---

# ğŸ”§ Extending the Model

This project is intentionally simple so you can extend it in many directions:

### â Architecture Improvements

* Token-level tokenizer (`tiktoken`, HuggingFace)
* RMSNorm
* RoPE (rotary positional embeddings)
* SwiGLU feed-forward networks
* FlashAttention
* Multi-GPU data parallel training

### â Training Enhancements

* Checkpoint saving/loading
* Larger datasets
* Curriculum training
* Mixed-precision training (FP16/BF16)

### â Integrations

* **RAG (Retrieval-Augmented Generation)**
* **IBM MQ message-driven inference**
* REST APIs (FastAPI)
* Web UI (Gradio)
* CI/CD pipelines for model updates

### â Productionizing

* Model serving
* Logging & evaluation dashboard
* Automatic prompt scaffolding
* Fine-tuning utilities

If you want, I can generate any of these extensions for your repo.

---

# ğŸ“œ License

MIT License â€“ free for personal and commercial use.
